---
layout: index

prev: 1.Intro:concepto_y_soporte_fisico
next: 3.Uso_de_sistemas

---

Tema 2: Técnicas de virtualización
==

<!--@
prev: 1.Intro:concepto_y_soporte_fisico
next: 3.Uso_de_sistemas
-->

<div class="objetivos" markdown="1">

<h2>Objetivos</h2>

1.  Conocer las diferentes tecnologías y herramientas de
virtualización tanto para procesamiento, comunicación y
almacenamiento. 

2. Realizar tareas de administración en infraestructura virtual.

</div>


Introducción
------------------

Una vez vista la introducción a la infraestructura virtual y algunas
técnicas usadas por la misma, en este tema pondremos en práctica lo
anterior trabajando con diferentes técnicas de virtualización a nivel
de sistema operativo y a nivel de hardware.

Comenzaremos por aprender a usar *contenedores*, un entorno de
virtualización a nivel de sistema operativo que permite trabajar con
muchas de las técnicas habituales en virtualización sin introducir un
*overhead* excesivo; previamente veremos los mecanismos incluidos en
el núcleo que permiten construirlos.

Espacios de nombres
-------------------------------

Los
[espacios de nombres o *namespaces* del núcleo](http://lwn.net/Articles/531114/)
son un mecanismo que permite aislar el identificador asignado a una
serie de recursos del resto del sistema. Por ejemplo, se puede
identificar un número de proceso (PID) independiente, que puede
coincidir con otro existente en otro espacio de nombres, o se puede
montar un recurso de forma que sea invisible al resto del sistema o
simplemente tenga un nombre diferente.

Hay seis tipos de *namespaces*, algunso de los cuales son
relativamente modernos y otros proceden de las versiones 2.4 y 2.6 del
núcleo:

* De montaje, aislan los recursos declarados con `mount`
* UTS (el acrónimo viene de *Unix Time Sharing System*, sistemas de
  virtualización tempranos), básicamente los nombres del ordenador y
  su dominio
* IPC o *inter-process communication*, referidos a los *sockets* y
    colas de mensajes
* PID o identificadores de proceso
* Red, los recursos relacionados con la red, números de puerto y
      demás.
* Usuario, lo que puede permitir, por ejemplo, que un proceso
        tenga privilegios de `root` dentro del espacio de nombres de
        un usuario y no los tenga fuera, creando contenedores de
        recursos.
		
La mayor parte de estos espacios de nombres se pueden asignar a una
nueva orden usando una llamada del sistema `CLONE`. Pero desde línea
de órdenes se pueden crear diferentes espacios de nombres usando
`unshare`, que está dentro del paquete `util-linux` (es posible que se
llame de otra forma en diferentes distribuciones) como
[cuentan en este blog](http://karelzak.blogspot.com.es/2009/12/unshare1.html). Por
ejemplo, podemos cambiar el nombre de la máquina:

![Usando lxc-chkconfig](../img/unshare.png)

Primero, se ha usado

	sudo unshare -u /bin/bash
   
En este caso, `-u` indica que vamos a crear un nuevo *namespace* UTS,
que nos permite cambiar el nombre de la máquina. Eso es lo que
hacemos: cambiamos el nombre de la máquina, y comprobamos que al salir
de la orden que había ejecutado, el intérprete de órdenes, se vuelve a
restaurar el nombre original. 

<div class='ejercicios' markdown="1">

Crear un espacio de nombres y montar en él una imagen ISO de un CD de
forma que no se pueda leer más que desde él. *Pista*: en
[ServerFault](http://serverfault.com/questions/198135/how-to-mount-an-iso-file-in-linux)
nos explican como hacerlo, usando el dispositivo *loopback*

</div>

El mecanismo de espacios de nombres es diferente al usado en 
[`cgroups`](1.Intro:concepto_y_soporte_fisico#restriccin_y_medicin_del_uso_de_recursos_),
tal como se vio en el tema anterior: teóricamente, un PID dentro de un
CGROUP es visible a todos los demás procesos; sin embargo, es
complementario porque mientras que uno aisla la visibilidad o el
ámbito otro aisla o limita el uso de recursos. Por ello constituyen la
base de los contenedores que se verán en este tema. 

`unshare`tiene sus limitaciones, y la principal es que sólo se puede
*entrar* en un *namespace* ejecutando un comando, no "desde fuera". A
partir de la versión 2.23 de util-linux (la versión en mi Ubuntu 12.04
es la 2.20) [un nuevo comando `nsenter`](http://karelzak.blogspot.com.es/2013/04/umount8-mount8-and-nsenter1.html) permitirá entrar dando el
PID del proceso dentro del que se haya creado. 

Puentes de red
----------------------

Tras las técnicas que se han visto de aislamiento y
compartimentalización de recursos hace falta ver alguna forma de crear
una *tarjeta de red virtual* para esos recursos, de forma que puedan
conectarse al exterior a través de la tarjeta de red del ordenador
anfitrión o entre sí entre diferentes máquinas virtuales del mismo
anfitrión. Además de actuar como tal, el interfaz de red virtual
tendrá que actuar como *puente*, enrutando todos los paquetes Ethernet
del invitado al anfitrió o a donde corresponda. Por eso las máquinas
virtuales usan interfaces de red virtuales llamados *puentes*. Para
usarlos necesitaremos instalar un [paquete de linux (y sus
dependencias) denominado `bridge-utils`](http://www.linuxfromscratch.org/blfs/view/svn/basicnet/bridge-utils.html).

La principal orden que provee este paquete es [`brctl` que podemos usar
directamente](https://wiki.debian.org/BridgeNetworkConnections) para crear este puente.

	sudo brctl addbr alcantara

Hace falta privilegios de superusuario para crear este nuevo interfaz;
una vez creado, 

	 ip addr show

nos mostrará, entre otras cosas

	alcantara: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN 
    link/ether 0a:f5:42:80:e7:09 brd ff:ff:ff:ff:ff:ff
	
en este instante ni está activado ni, en realidad, hace nada: no tiene
dirección ethernet, aunque sí un MAC propio. Este puente podemos, por
ejemplo, añadirlo a otro interfaz como el eth0 típico de cualquier
ordenador:

	sudo brctl addif alcantara eth0
	
Si mostramos de nuevo los interfaces con `ip addr show`, ahora
mostrará:

	eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast	master alcantara state UP qlen 1000
	
es decir, el nombre del puente aparecerá en la descripción del mismo y
en el estado del puente aparecerá la MAC del interfaz al que está
conectada.

Ahora este interfaz actuará como cualquier otro a la hora de
configurarlo: podemos asignarle una IP fija o asignársela con DHCP
cuando se *levante*, pero en cuanlquier caso podremos usarlo como una
tarjeta de red virtual para las máquinas virtuales que vayamos a
usar. Es posible que si hemos instalado algún paquete de
virtualización ya tengamos alguna creada, `brctl show` muestra todos
los puentes que existen en una máquina, por ejemplo:

	bridge name	bridge id		STP enabled	interfaces
	alcantara		8000.1c6f65a40690	no		eth2
	lxcbr0		8000.000000000000	no		
	virbr0		8000.000000000000	yes		
	
Que muestra los puentes creados por `lxc` (que veremos más adelante) y
por VirtualBox.

<div class='ejercicios' markdown="1">

1. Mostrar los puentes configurados en el sistema operativo.

2. Crear un interfaz virtual y asignarlo al interfaz de la tarjeta
wifi, si se tiene, o del fijo, si no se tiene.

</div>

La utilidad de la creación de puentes no se limita a su uso por
máquinas virtuales. Se puede usar, por ejemplo, para
[equilibara la carga entre dos interfaces](http://archive09.linux.com/feature/133849)
o simplemente crear un sólo interfaz virtual que contenga los dos
interfaces reales presentes en el ordenador; también para simular
redes dentro de un sólo ordenador. 

Creando el contenido de nuevas máquinas y metiéndolos en jaulas
--------------------------------------------------------------

En el mundo del software libre y de las distribuciones de Linux
siempre es fácil añadir el contenido a una nueva máquina virtual; dado
que, en general, es "como" una máquina real basta con "arrancar" con
una ISO y llevar a cabo el proceso habitual de instalación. Sin
embargo, hacerlo conlleva una serie de problemas, el principal es que
es muy difícil que una instalación no necesite una parte interactiva
(nombres de usuario y claves, por ejemplo) y que, además, no se puede
controlar el proceso fácilmente desde otro sistema.

Además, en muchos casos no hace falta instalar un sistema completo,
sino sólo una parte; en alternativas de virtualización a nivel de
sistema operativo, como las que vamos a ver, hay partes del sistema
(núcleo, dispositivos) que se comparten y sólo hace falta instalar las
órdenes mínimas para dotar a la *máquina* de una serie de recursos
para funcionar.

por supuesto, una de las formas de hacerlo es simplemente ir copiendo
las partes necesarias del sistema de ficheros raíz al nuevo
sistema. Pero hay una serie de utilidades en Linux que lo hacen más
fácil. En el mundo Debian (que incluye Debian, Ubuntu y Guadalinex) se
usa `debootstrap`.

Una vez instalado, se puede usar de esta forma
	
		sudo debootstrap --arch=amd64 quantal /home/jaulas/quantal/	http://archive.ubuntu.com/ubuntu

La primera parte indica el tipo de arquitectura que se va a usar. Una
de las ventajas que tiene `debootstrap` es que puedes crear
instalaciones de 32 bits (`arch=i386`) dentro de un anfitrión de 64
bits (al revés, no, claro). El segundo argumento es el nombre de la
distro que se va a buscar en el repositorio, en este caso Quantal Quetzal. A continuación, el
directorio en el que lo vamos a instalar, que habremos creado
previamente, y finalmente la dirección del repositorio, que en este
caso de la de Ubuntu; la de Debian sería
`http://ftp.debian.org/debian/` y en el caso de Guadalinex sería un
tanto diferente, con diferentes directorios para cada distro, por
ejemplo `http://ftp.cica.es/Guadalinex/guadalinex-buho/`para Búho, la
última. 


Un primer paso de virtualización: *contenedores*
-------

El aislamiento de grupos de procesos formando una *jaula* o
*contenedor* ha sido una característica de ciertos sistemas operativos
de la rama Unix desde los años 80, en forma del programa
[chroot](http://es.wikipedia.org/wiki/Chroot) (creado por Bill Joy, el
que más adelante sería uno de los padres de Java). La limitación de
uso de recursos de las *jaulas `chroot`* se limitaba a la protección
del acceso a ciertos recursos del sistema de archivos, aunque son
relativamente fáciles de superar; incluso así, fue durante mucho
tiempo la forma principal de configurar servidores de alojamiento
compartidos. Las
[jaulas BSD](http://en.wikipedia.org/wiki/FreeBSD_jail) constituían un
sistema más avanzado, implementando una
[virtualización a nivel de sistema operativo](http://en.wikipedia.org/wiki/Operating_system-level_virtualization)
que creaba un entorno virtual prácticamente indistinguible de una
máquina real (o máquina virtual real). Estas *jaulas* no sólo impiden
el acceso a ciertas partes del sistema de ficheros, sino que también
restringían lo que los procesos podían hacer en relación con el resto
del sistema. Tiene como limitación, sin embargo, la obligación de
ejecutar la misma versión del núcleo del sistema.

El mundo Linux no tendría capacidades similares hasta los años 80, con
[vServers, OpenVZ y finalmente LXC](http://en.wikipedia.org/wiki/Operating_system-level_virtualization#Implementations). Este
último, [LXC](http://lxc.sourceforge.net), se basa en el concepto de
[grupos de control o CGROUPS](http://en.wikipedia.org/wiki/Cgroups),
una capacidad del núcleo de Linux desde la versión 2.6.24 que crea
*contenedores* de procesos unificando diferentes capacidades del
sistema operativo que incluyen acceso a recursos, prioridades y
control de los procesos. Los procesos dentro de un contenedor están
*aislados* de forma que sólo pueden *ver* los procesos dentro del
mismo, creando un entorno mucho más seguro que las anteriores
*jaulas*.

Dentro de la familia de sistemas operativos Solaris (cuya última
versión libre se denomina
[illumos](http://en.wikipedia.org/wiki/Illumos), y tiene también otras
versiones como SmartOS) la tecnología
correspondiente se denomina
[zonas](http://en.wikipedia.org/wiki/Solaris_Zones). La principal
diferencia es el bajo *overhead* que le añaden al sistema operativo y
el hecho de que se les puedan asignar recursos específicos; estas
diferencias son muy leves al tratarse simplemente de otra
implementación de virtualización a nivel de sistema operativo.

Un contenedor es una forma *ligera* de virtualización, en el sentido
que no requiere un hipervisor para funcionar ni, en principio, ninguno
de los mecanismos hardware necesarios para llevar a cabo
virtualización. Tiene la limitación de que la *máquina invitada* debe
tener el mismo kernel y misma CPU que la máquina anfitriona, pero si
esto no es un problema, puede resultar una alternativa útil y ligera a
la misma.

<div class='ejercicios' markdown="1">
Instala LXC en tu versión de Linux favorita.
</div>

No todos los núcleos pueden usar este tipo de container; para empezar,
dependerá de cómo esté compilado, pero también del soporte que tenga
el hardware. `lxc-checkconfig` permite comprobar si está preparado
para usar este tipo de tecnología. Parte de la configuración se
refiere a la instalación de `cgroups`, que hemos visto antes; el resto
a los espacios de nombres y a capacidades *misceláneas* relacionadas
con la red y el sistema de ficheros. 

![Usando lxc-chkconfig](../img/lxcchkconfig.png)
